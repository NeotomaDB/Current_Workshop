---
title: "A Not so Simple Workflow - SDM Version"
author: "Nora Schlenker, Simon Goring, Socorro Dominguez Vidaña"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    fig_caption: yes
    keep_md: yes
    self_contained: yes
    theme: readable
    toc: yes
    toc_float: yes
    css: "text.css"
  pdf_document:
    pandoc_args: "-V geometry:vmargin=1in -V geometry:hmargin=1in"
dev: svg
highlight: tango
---

## Building New Chronologies

This RMarkdown document will walk you through the process of:

1.  Downloading pollen records from multiple sites
2.  Filtering for specific taxa and taxonomic harmonization
3.  Filtering and binning for specific time periods
4.  Linking to environmental data
5.  Performing simple SDMs for different time periods

## Load Libraries

For this workshop element we only need quite a few packages. We'll be loading multiple records from Neotoma, filtering by taxa and time periods, and preforming simple species distrubution models.

We'll be using the R package `pacman` here, to automatically load and install packages:

```{r setup}
pacman::p_load(neotoma2, dplyr, ggplot2)
```

## Loading Datasets [.tabset]

We worked through the process for finding and downloading records using `neotoma2` in the [previous workshop](https://open.neotomadb.org/Current_Workshop/simple_workflow.html). For this exercise we will be pulling pollen records from all of Europe to contstruct species distribution models for select species for different time periods. The following code defines our region of interest (a simple polygon of Europe) and downloads the sites, dataset, and downloads for all pollen records that are not missing an age depth model. This is the same workflow as the simple workflow we completed in the morning session but condensed to a few lines of code. Downloading this volume of data from Neotoma can take a long time and is computationally expensive for the database itself so we have already completed the data download and here you will read in the data from our data folder.Under the results you can take a look at all the sites we downloaded. 

#### Code 
```{r geteurope, message = FALSE, eval = FALSE}
europe <-  '{"type": "Polygon",
"coordinates": [[
 [ -32.13,66.46],
 [-14.09,36.93],
 [30.16,34.71],
 [35.79,69.17],
 [-32.13,66.46]]]}'
europe_sf <- geojsonsf::geojson_sf(europe)

## The following lines of code are commented out because we've already run it for you to speed up the process. 
# Downloading large amounts of data can take a long time.
# europe_sites <- neotoma2::get_sites(loc = europe, datasettype = "pollen", all_data = TRUE)
# europe_datasets <- neotoma2::get_datasets(loc = europe, datasettype = "pollen", all_data = TRUE)
# europe_records <- europe_datasets %>% 
#   neotoma2::filter(!is.na(age_range_young))
# europe_downloads <- europe_records %>% get_downloads(all_data = TRUE)
# europe_samples <- samples(europe_downloads)
# 
# saveRDS(europe_sites, "data/europe_sites.RDS")
# saveRDS(europe_datasets, "data/europe_datasets.RDS")
# saveRDS(europe_downloads, "data/europe_downloads.RDS")
# saveRDS(europe_samples, "data/europe_samples.RDS")

europe_sites <- readRDS("data/europe_sites.RDS")
europe_datasets <- readRDS("data/europe_datasets.RDS")
europe_downloads <- readRDS("data/europe_downloads.RDS")
europe_samples <- readRDS("data/europe_samples.RDS")

```

#### Sites Map
```{r geteuropeshow, eval=TRUE, echo = FALSE}
neotoma2::plotLeaflet(europe_sites) %>% 
  leaflet::addPolygons(map = ., 
                       data = europe_sf, 
                       color = "green")

```

#### Data table

```{r downloadsCode, echo = FALSE}
europe_samples[1:10,] %>% dplyr::select(age, variablename, ecologicalgroup, value, units, sitename, lat, long) %>% 
  DT::datatable(data = ., 
                options = list(scrollX = "100%", dom = 't'))
```

### Data exploration and complete taxon filtering and harmonization {.tabset}

Next we are going to 

#### Code
```{r filtertaxa, message = FALSE, eval = FALSE}
europe_subset <- europe_samples %>%
  filter(stringr::str_detect(variablename, "Fagus*") | stringr::str_detect(variablename, "Picea*") | stringr::str_detect(variablename, "Corylus*")) %>%
  mutate(variablename = replace(variablename, stringr::str_detect(variablename, "Fagus*"), "Fagus"), 
         variablename = replace(variablename, stringr::str_detect(variablename, "Picea*"), "Picea"), 
         variablename = replace(variablename, stringr::str_detect(variablename, "Corylus*"), "Corylus"))
```

#### Results
```{r filtertaxaShow, eval = TRUE, message = FALSE, echo = FALSE}
europe_subset[1:10,] %>% dplyr::select(age, variablename, ecologicalgroup, value, units, sitename, lat, long) %>% 
  DT::datatable(data = ., 
                options = list(scrollX = "100%", dom = 't'))
```

We can look at other tools to decided how we want to manage the chroncontrols, for example, saving them and editing them using Excel or another spreadsheet program. We could add a new date by adding a new row. In this example we're just going to modify the existing ages to provide better constraints at the core top. We are setting the core top to *-55 calibrated years BP*, and assuming an uncertainty of 2 years, and a thickness of 2cm.

This generally won't change too much, and I have no real basis for doing this explicitly, but this is simply for illustration.

To do these assignments we're just directly modifying cells within the `controls` `data.frame`:

```{r modifyControls, message = FALSE, eval=FALSE}
# Directly assign the values
controls$chroncontrolage[1] <- 20000	
controls$agelimityounger[1] <- 15000
controls$agelimitolder[1] <- 29000
controls$thickness[1] <- 16
```

```{r modifyControlsShow, message = FALSE, eval = TRUE, echo=FALSE}
controls$chroncontrolage[1] <- 20000	
controls$agelimityounger[1] <- 15000
controls$agelimitolder[1] <- 29000
controls$thickness[1] <- 16
controls %>% DT::datatable(data = ., 
                options = list(scrollX = "100%"))
```

### Extract Depth & Analysis Unit IDs

Once our `chroncontrols` table is updated, we extract the `depth`s and `analysisunitid`s from the dataset `samples()`. Pulling in both `depth`s and `analysisunitid`s is important because a single collection unit may have multiple datasets, which may have non-overlapping depth sequences. So, when adding sample ages back to a record we use the `analysisunitid` to make sure we are providing the correct assignment since depth may be specific to a single dataset.

```{r predictDepths, message = FALSE, results="hide"}
# Get a two column data.frame with columns depth and analysisunitid.
# Sort the table by depth from top to bottom for "Bchronology"
predictDepths <- samples(monticchio) %>%
  select(depth, analysisunitid) %>% 
  unique() %>% 
  arrange(depth)

# Pass the values from `controls`. We're assuming the difference between
# chroncontrolage and the agelimityounger is 1 SD.
# Note that for the parameter 'calCurves' we are using a "normal" 
# distribution for the modern sample (core top) and choosing the
# IntCal20 curve for the other two radiocarbon dates.

newChron <- Bchron::Bchronology(ages = controls$chroncontrolage,
                                ageSds = abs(controls$agelimityounger - 
                                               controls$chroncontrolage),
                                calCurves = c("normal", rep("intcal20", 5)),
                                positionThicknesses = controls$thickness,
                                positions = controls$depth,
                                predictPositions = predictDepths$depth,
                                allowOutside = TRUE,
                                ids = controls$chroncontrolid)

# Predict ages at each depth for which we have samples.  Returns a matrix.
newpredictions <- predict(newChron, predictDepths$depth)
```

```{r chronologyPlot, fig.cap="Age-depth model for Stará Boleslav, with probability distributions superimposed on the figure at each chronology control depth."}
plot(newChron) +
  ggplot2::labs(
    xlab = "Age (cal years BP)",
    ylab = "Depth (cm)"
  )
```

### Creating the New `chronology` and `contact` objects

Given the new chronology, we want to add it to the `sites` object so that it becomes the default for any calls to `samples()`. To create the metadata for the new chronology, we use `set_chronology()` using the properties from the [`chronology` table in Neotoma](https://open.neotomadb.org/dbschema/tables/chronologies.html):

```{r createChronology, message = FALSE}
# Add information about the people who generated the new chronology:
creators <- c(set_contact(givennames = "Simon James",
                          familyname = "Goring",
                          ORCID = "0000-0002-2700-4605"),
              set_contact(givennames = "Socorro",
                          familyname = "Dominguez Vidaña",
                          ORCID = "0000-0002-7926-4935"))

# Add information about the chronology:
newChronmonticchio <- set_chronology(agemodel = "Bchron model",
                                contact = creators,
                                isdefault = 1,
                                ageboundolder = max(newpredictions),
                                ageboundyounger = min(newpredictions),
                                dateprepared = lubridate::today(),
                                modelagetype = "Calendar years BP",
                                chronologyname = "Simon's example chronology",
                                chroncontrols = controls)


newChronmonticchio$notes <- 'newChron <- Bchron::Bchronology(ages = controls$chroncontrolage,
                                ageSds = abs(controls$agelimityounger - 
                                               controls$chroncontrolage),
                                calCurves = c("normal", rep("intcal20", 2)),
                                positionThicknesses = controls$thickness,
                                positions = controls$depth,
                                allowOutside = TRUE,
                                ids = controls$chroncontrolid,
                                predictPositions = predictDepths)'
```

### Adding the `chronology` to the `collectionunit`

Once we've created the chronology we need to apply it back to the collectionunit. We also need to add the predicted dates into the samples for each dataset associated with the collectionunit.

So:

1.  we have a collectionunit in `monticchio` that is accessible at `monticchio[[1]]$collunits`.
2.  We can use the function `add_chronology()`, which takes the chronology object and a `data.frame()` of sample ages.
3.  The predicted dates associated with the new chronology need to be transferred to each `samples` object within the `collectionunit`.

This is all bound up in the `add_chronology()` function, which takes the `collectionunit`, modifys it, and returns the newly updated `collectionunit`.

```{r addChronology, message = FALSE}
newSampleAges <- data.frame(predictDepths,
                            age = colMeans(newpredictions),
                            ageolder = colMeans(newpredictions) + 
                              apply(newpredictions, 2, sd),
                            ageyounger = colMeans(newpredictions) - 
                              apply(newpredictions, 2, sd),
                            agetype = "Calendar years")

monticchio[[1]]$collunits[[1]] <- add_chronology(monticchio[[1]]$collunits[[1]], 
                                            newChronmonticchio, 
                                            newSampleAges)
```

With this, we now have the updated collectionunit. Lets take a look at how this affects the age model overal. To pull the ages from the prior chronologies, we use the `set_default()` function to change the default chronology, and then extract ages, depths & analysisunits:

```{r getAgesfromChronologies}
# The new chronology is currently the default chronology.
newages <- samples(monticchio) %>%
  select(depth, analysisunitid, age) %>% 
  unique() %>% 
  arrange(depth) %>% 
  mutate(agecat = "new")

monticchio[[1]]$collunits[[1]]$chronologies <- set_default(monticchio[[1]]$collunits[[1]]$chronologies,
                                                      25746)
plotforages <- samples(monticchio) %>%
  select(depth, analysisunitid, age) %>% 
  unique() %>% 
  arrange(depth) %>% 
  mutate(agecat = "old") %>% 
  bind_rows(newages)

```

And we can look at the difference visually:

```{r plotAgeDifferences, fig.cap="Differences in age representation between chronologies between existing chronologies and the new Bchron chronology."}
ggplot(plotforages, aes(x = depth, y = age)) +
  geom_path(aes(color = agecat)) +
  theme_bw() +
  xlab("Depth (cm)") +
  ylab("Calibrated Years BP")
```

So we can see the impact of the new chronology on the age model for the record, and we can make choices as to which model we want to use going forward. We can use this approach to create multiple new chronologies for a single record, tuning parameters within `Bchronology()`, or using Bacon and different parameters. Because the `chronology` is an R object we can save the objects for use in future sessions, and associate them with existin records, or we can re-run the models again.

## Summary

From this notebook we have learned how to:

1.  Download a single record (the monticchio record using `get_downloads()`)
2.  Examining the chronologies for the record (using `chronologies()` and associated chronological controls (using `chroncontrols()`)
3.  Creating a new chronology for the record (using `set_chronology()`)
4.  Adding the chronology to the record (using `add_chronology()`)
5.  Switching between default chronologies (using `set_default()`)

This approach is focused on a single record, but much of what is done here can be extended to multiple records using functions. We hope it's been helpful!
